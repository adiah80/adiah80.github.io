{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Task   : Normalizing Flows for Fast Detector Simulation (Task 2)\n",
    "Author : Aditya Ahuja \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:39.016420Z",
     "start_time": "2021-04-12T16:41:39.013569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Flags\n",
    "DO_CACHE = False      # Recreate all the caches ?\n",
    "DO_ANALYSIS = False   # Show all the dataset analysis ?\n",
    "\n",
    "# Root task folder.\n",
    "import os\n",
    "ROOT = os.path.dirname(os.getcwd())+ '/' \n",
    "\n",
    "# Input format of files in $ROOT/data\n",
    "INPUT_FORMAT = 'Boosted_Jets_Sample-{}.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:44.607301Z",
     "start_time": "2021-04-12T16:41:40.112327Z"
    }
   },
   "outputs": [],
   "source": [
    "''' DOING IMPORTS '''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# Set Numpy Print Options\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:44.820405Z",
     "start_time": "2021-04-12T16:41:44.608981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Structure [Excluding Log/Temp Files]:\n",
      "\u001b[01;34m.\u001b[00m\n",
      "├── \u001b[01;34mcache\u001b[00m\n",
      "│   ├── X_dict.pkl\n",
      "│   ├── X_dict_0.pkl\n",
      "│   ├── X_dict_1.pkl\n",
      "│   ├── X_dict_2.pkl\n",
      "│   ├── X_dict_3.pkl\n",
      "│   └── X_dict_4.pkl\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── Boosted_Jets_Sample-0.snappy.parquet\n",
      "│   ├── Boosted_Jets_Sample-1.snappy.parquet\n",
      "│   ├── Boosted_Jets_Sample-2.snappy.parquet\n",
      "│   ├── Boosted_Jets_Sample-3.snappy.parquet\n",
      "│   └── Boosted_Jets_Sample-4.snappy.parquet\n",
      "├── \u001b[01;32mget_dataset.py\u001b[00m\n",
      "├── \u001b[01;34mlogs\u001b[00m\n",
      "└── \u001b[01;34mnbs\u001b[00m\n",
      "    └── task2.ipynb\n",
      "\n",
      "4 directories, 13 files\n"
     ]
    }
   ],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "# ROOT = os.path.dirname(os.getcwd()) + '/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "\n",
    "print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "! tree -I 'model*|temp__*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:48.689455Z",
     "start_time": "2021-04-12T16:41:48.683851Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCTIONS: DATA/INSTANCE LOADING '''\n",
    "\n",
    "def load_dataset(dataset_file):\n",
    "    f_path = DATA_ROOT + dataset_file\n",
    "    data = pq.read_table(f_path)\n",
    "    return data\n",
    "        \n",
    "def get_instance(data, idx):\n",
    "    assert idx < 32000\n",
    "    instance = np.array(data[0][idx].as_py())\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:49.400375Z",
     "start_time": "2021-04-12T16:41:49.386773Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCTIONS: PARSING/VISUALIZING IMAGES '''\n",
    "\n",
    "def parse_img(track_img, reduce=False):\n",
    "    if reduce:\n",
    "        track_img = cv2.resize(track_img, dsize=(25,25))\n",
    "        # plt.imshow(track_img)\n",
    "        \n",
    "    x_pos = []\n",
    "    y_pos = []\n",
    "    val = []\n",
    "    for x_idx in range(track_img.shape[0]):\n",
    "        for y_idx in range(track_img.shape[1]):\n",
    "            if track_img[x_idx][y_idx] != 0:\n",
    "                val.append(track_img[x_idx][y_idx])\n",
    "                x_pos.append(x_idx)\n",
    "                y_pos.append(y_idx)\n",
    "    \n",
    "    if reduce:\n",
    "        x_pos = [5*v for v in x_pos]\n",
    "        y_pos = [5*v for v in y_pos]\n",
    "    # print(len(x_pos), len(y_pos), len(val))\n",
    "    return x_pos, y_pos, val\n",
    "\n",
    "def vis(img, title=None, scale=1000, cmap='gist_heat', reduce=False):        \n",
    "    x_pos, y_pos, val = parse_img(img, reduce)\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val))*scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[14,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 125)\n",
    "    plt.ylim(0, 125)\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:50.878269Z",
     "start_time": "2021-04-12T16:41:50.871496Z"
    }
   },
   "outputs": [],
   "source": [
    "''' ANALYSING PARSED IMAGES '''\n",
    "\n",
    "if DO_ANALYSIS:\n",
    "    # data = load_dataset(INPUT_FORMAT.format(0))\n",
    "    \n",
    "    img = get_instance(data, 2)\n",
    "    vis(img[0], scale=500, title='Track')\n",
    "    vis(img[1], scale=200, title='ECAL')\n",
    "    vis(img[2], scale=300, title='HCAL', reduce=True)\n",
    "    \n",
    "    # img = get_instance(data, 0)\n",
    "    # rc_plot([img[0], img[1], img[2]], labels = ['idx=0', 'idx=1', 'idx=2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Parsed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_track`: \n",
    "   - Array containing information about each events `track` channel.\n",
    "   - Each element in `X_track` contains three arrays (of equal lengths) corresponding to `(X position, Y position, Value)`.\n",
    "   - Each element in these arrays contains an instance's respective value. \n",
    "\n",
    "```\n",
    "X_track : [\n",
    "    [X_position: [], Y_position: [], Value: []],           # Event 1\n",
    "    [X_position: [], Y_position: [], Value: []],           # Event 2\n",
    "    [X_position: [], Y_position: [], Value: []],           # Event 3\n",
    "    ...\n",
    "    [X_position: [], Y_position: [], Value: []],           # Event n\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "Similarly we construct arrays `X_ECAL` and `X_HCAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:55.063673Z",
     "start_time": "2021-04-12T16:41:55.051362Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCTIONS: PARSING/MERGING/CACHING DATASETS '''\n",
    "\n",
    "def cache_dataset(file_name, X_track, X_ECAL, X_HCAL):\n",
    "    X_dict = {\n",
    "        'X_track': X_track,\n",
    "        'X_ECAL': X_ECAL,\n",
    "        'X_HCAL': X_HCAL,\n",
    "    }\n",
    "    with open(CACHE_ROOT + file_name, 'wb') as f:\n",
    "        pkl.dump(X_dict, f)\n",
    "        \n",
    "def load_cached_dataset(file_name):\n",
    "    with open(CACHE_ROOT + file_name, 'rb') as f:\n",
    "        X_dict = pkl.load(f)\n",
    "    X_track = X_dict['X_track']\n",
    "    X_ECAL = X_dict['X_ECAL']\n",
    "    X_HCAL = X_dict['X_HCAL']\n",
    "    return X_track, X_ECAL, X_HCAL\n",
    "\n",
    "def parse_dataset(data, save_datafile, reduce):\n",
    "    X_track = []\n",
    "    X_ECAL = []\n",
    "    X_HCAL = []\n",
    "\n",
    "    for i in trange(len(data)):\n",
    "        img = get_instance(data, i)\n",
    "        X_track.append(parse_img(img[0]))\n",
    "        X_ECAL.append(parse_img(img[1]))\n",
    "        X_HCAL.append(parse_img(img[2], reduce=reduce))\n",
    "\n",
    "    # print(len(X_track), len(X_ECAL), len(X_HCAL))\n",
    "    cache_dataset(save_datafile, X_track, X_ECAL, X_HCAL)\n",
    "\n",
    "def merge_parsed_datasets(CACHE_FORMAT, OUTPUT_FILE):\n",
    "    X_track = []\n",
    "    X_ECAL = []\n",
    "    X_HCAL = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        data_file = CACHE_FORMAT.format(i)\n",
    "        X_track_t, X_ECAL_t, X_HCAL_t = load_cached_dataset(data_file)\n",
    "        # print(len(X_track_t), len(X_ECAL_t), len(X_HCAL_t))\n",
    "        X_track += X_track_t\n",
    "        X_ECAL += X_ECAL_t\n",
    "        X_HCAL += X_HCAL_t\n",
    "    \n",
    "    # print(len(X_track), len(X_ECAL), len(X_HCAL))\n",
    "    cache_dataset(OUTPUT_FILE, X_track, X_ECAL, X_HCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:41:55.786999Z",
     "start_time": "2021-04-12T16:41:55.781858Z"
    }
   },
   "outputs": [],
   "source": [
    "''' CACHING ALL 5 DATASETS INTO A SINGLE DATASET '''\n",
    "\n",
    "CACHE_FORMAT = 'X_dict_{}.pkl'\n",
    "OUTPUT_FILE = 'X_dict.pkl'\n",
    "\n",
    "if DO_CACHE:\n",
    "    for i in range(5):\n",
    "        load_datafile = INPUT_FORMAT.format(i)\n",
    "        save_datafile = CACHE_FORMAT.format(i)\n",
    "        data = load_dataset(load_datafile)\n",
    "        parse_dataset(data, save_datafile, reduce=True)\n",
    "    merge_parsed_datasets(CACHE_FORMAT, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:42:43.670322Z",
     "start_time": "2021-04-12T16:41:56.416004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000 160000 160000\n"
     ]
    }
   ],
   "source": [
    "''' LOADING THE SINGLE PARSED DATASET '''\n",
    "\n",
    "X_track, X_ECAL, X_HCAL = load_cached_dataset('X_dict.pkl')\n",
    "print(len(X_track), len(X_ECAL), len(X_HCAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:42:43.677555Z",
     "start_time": "2021-04-12T16:42:43.672114Z"
    }
   },
   "outputs": [],
   "source": [
    "''' ANALYSIS OF PARSED DATASET '''\n",
    "\n",
    "def analyse_dataset(Xs, labels):\n",
    "    l = len(Xs[0])\n",
    "\n",
    "    for i, Xi in enumerate(Xs):\n",
    "        plt.figure(figsize=[16,3])\n",
    "\n",
    "        X_lens = sorted([len(t[0]) for t in Xi])[:l]\n",
    "        X_points = np.arange(0, l+0.1, l/10)\n",
    "        cutoff = X_lens[int(X_points[9])]\n",
    "\n",
    "        title = '[{}] Proposed Cutoff: {}'.format(labels[i], cutoff)\n",
    "        plt.title(title, size=18)\n",
    "        plt.plot(X_lens)\n",
    "        plt.vlines(X_points, ymin=0, ymax=max(X_lens), \n",
    "                   linestyles='--', color='r', label='percentiles')\n",
    "        plt.plot(X_points[9], cutoff, 'xk', markersize=20, \n",
    "                 label='Cutoff Point')\n",
    "\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()    \n",
    "        \n",
    "if DO_ANALYSIS:\n",
    "    analyse_dataset([X_track, X_ECAL, X_HCAL], \n",
    "                    labels=['X_track', 'X_ECAL', 'X_HCAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T16:42:43.777128Z",
     "start_time": "2021-04-12T16:42:43.679056Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCTIONS: PAD/MERGE ALL THREE CHANNELS '''\n",
    "\n",
    "def pad_array(outer_array, output_len, pad_value=0):\n",
    "    outer_array_pad = []\n",
    "    for tri_array in tqdm(outer_array, leave=False):\n",
    "        tri_array_pad = []\n",
    "        for inner_arr in tri_array:\n",
    "            inner_arr = inner_arr[:output_len]\n",
    "            pad_len = output_len - len(inner_arr)\n",
    "            inner_arr_pad = np.pad(inner_arr, (0, pad_len))\n",
    "            tri_array_pad.append(inner_arr_pad)\n",
    "        outer_array_pad.append(tri_array_pad)\n",
    "    outer_array_pad = np.array(outer_array_pad)\n",
    "    return outer_array_pad\n",
    "\n",
    "def standardise_merge_arrays(X_track, X_ECAL, X_HCAL, MAX_INSTANCES):\n",
    "    X_track = pad_array(X_track, MAX_INSTANCES['X_track'])\n",
    "    X_ECAL = pad_array(X_ECAL, MAX_INSTANCES['X_ECAL'])\n",
    "    X_HCAL = pad_array(X_HCAL, MAX_INSTANCES['X_HCAL'])\n",
    "    \n",
    "    X = []\n",
    "    for i in range(len(X_track)):\n",
    "        X_instance = np.concatenate([X_track[i].flatten(), \n",
    "                                     X_ECAL[i].flatten(),\n",
    "                                     X_HCAL[i].flatten()])\n",
    "        X.append(X_instance)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    print('Max Instances: ', MAX_INSTANCES)\n",
    "    print('Shape of Individual arrays:', X_track.shape, X_ECAL.shape, X_HCAL.shape)\n",
    "    print('Shape of Combined array:', X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:00:07.608321Z",
     "start_time": "2021-04-12T16:58:41.907436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Instances:  {'X_track': 56, 'X_ECAL': 912, 'X_HCAL': 56}\n",
      "Shape of Individual arrays: (160000, 3, 56) (160000, 3, 912) (160000, 3, 56)\n",
      "Shape of Combined array: (160000, 3072)\n"
     ]
    }
   ],
   "source": [
    "''' MERGING ALL THREE CHANNELS INTO SINGLE ARRAY '''\n",
    "\n",
    "MAX_INSTANCES = {\n",
    "    'X_track': 56,   # 90 percentile is  47  \n",
    "    'X_ECAL': 912,   # 90 percentile is 898\n",
    "    'X_HCAL': 56,    # 90 percentile is  49\n",
    "}\n",
    "\n",
    "# Merge into single array\n",
    "X = standardise_merge_arrays(X_track, X_ECAL, X_HCAL, MAX_INSTANCES)\n",
    "\n",
    "# Delete previous arrays to save memory\n",
    "del X_track, X_ECAL, X_HCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T07:23:59.639843Z",
     "start_time": "2021-04-12T07:23:59.515917Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlining the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:00:16.988230Z",
     "start_time": "2021-04-12T17:00:07.613703Z"
    }
   },
   "outputs": [],
   "source": [
    "''' DOING IMPORTS '''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam, AdamW, RMSprop, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:00:16.995584Z",
     "start_time": "2021-04-12T17:00:16.990914Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCTIONS: SPLIT DATASET, CREATE DATALOADERS ''' \n",
    "\n",
    "def split_dataset(X, test_size = 0.1, valid_size = 0.1):\n",
    "    test_size1 = test_size\n",
    "    test_size2 = test_size / (1 - valid_size)   # Normalize\n",
    "    \n",
    "    X_train, X_test = train_test_split(X, \n",
    "                                       test_size=test_size1, \n",
    "                                       random_state=0)\n",
    "    X_train, X_valid = train_test_split(X_train, \n",
    "                                        test_size=test_size2, \n",
    "                                        random_state=0) \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def get_dataLoader(X, cfg):\n",
    "    ''' Constructs DataLoaders from (X) instances.\n",
    "    '''\n",
    "    dataset = TensorDataset(torch.Tensor(X)) \n",
    "    dataLoader = DataLoader(dataset, \n",
    "                            batch_size=cfg['batch_size'], \n",
    "                            num_workers=cfg['num_workers']) \n",
    "    return dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:00:17.109664Z",
     "start_time": "2021-04-12T17:00:16.997627Z"
    }
   },
   "outputs": [],
   "source": [
    "''' CLASS DEFINITION: PYTORCH AUTOENCODER MODEL '''\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_features, layers):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.layers = [n_features] + layers\n",
    "        self.rev_layers = self.layers[::-1]\n",
    "        self.n = len(self.layers)\n",
    "        \n",
    "        encoder_layers = []\n",
    "        for i in range(0, self.n-1):\n",
    "            en_layer = nn.Linear(self.layers[i], self.layers[i+1])\n",
    "            bn_layer = nn.BatchNorm1d(self.layers[i+1])\n",
    "            encoder_layers.append(en_layer)\n",
    "            if i != self.n-2 :\n",
    "                encoder_layers += [bn_layer, nn.ReLU()]\n",
    "        self.encoder = nn.ModuleList(encoder_layers)\n",
    "            \n",
    "        decoder_layers = []\n",
    "        for i in range(self.n-1, 0, -1):\n",
    "            de_layer = nn.Linear(self.layers[i], self.layers[i-1])\n",
    "            bn_layer = nn.BatchNorm1d(self.layers[i-1])\n",
    "            decoder_layers.append(de_layer)\n",
    "            if i != 1 :\n",
    "                decoder_layers += [bn_layer, nn.ReLU()]\n",
    "        self.decoder = nn.ModuleList(decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"LAYERS: \", self.layers)\n",
    "        print(\"\\nENCODER: \\n============\")\n",
    "        print(self.encoder)\n",
    "        print(\"\\nDECODER: \\n============\")\n",
    "        print(self.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:21:59.060714Z",
     "start_time": "2021-04-12T17:21:59.040586Z"
    }
   },
   "outputs": [],
   "source": [
    "''' CLASS DEFINITION: PYTORCH-LIGHTNING CLASSIFIER FOR TRAINING '''\n",
    "\n",
    "# Define the training logic.\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.save_hyperparameters(cfg)\n",
    "\n",
    "        n_features = sum([cfg['MAX_INSTANCES'][k] for k in cfg['MAX_INSTANCES']]) * 3\n",
    "        self.model = AutoEncoder(n_features, cfg['layers']).to(cfg['device'])\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.sched = cfg['scheduler']\n",
    "        self.lr = cfg['learning_rate']\n",
    "        self.opt = cfg['optimizer']\n",
    "        self.cfg = cfg\n",
    "        self.predictions = []\n",
    "        # print(self.cfg)\n",
    "        # self.model.describe()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X = batch[0]\n",
    "        preds = self.model(X)\n",
    "        loss = self.criterion(preds, X)\n",
    "        self.log('Train_Loss', loss.detach(), \n",
    "                 on_step=True, on_epoch=True, prog_bar=True)  \n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X = batch[0]\n",
    "        preds = self.model(X)\n",
    "        loss = self.criterion(preds, X)\n",
    "        self.log('Valid_Loss', loss.detach(), \n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X = batch[0]\n",
    "        preds = self.model(X)\n",
    "        self.predictions += preds.detach().cpu()\n",
    "        loss = self.criterion(preds, X)\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        self.log('avg_test_loss', avg_loss)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        assert self.lr is not None\n",
    "        \n",
    "        # Optimizer\n",
    "        if self.opt == 'adam':\n",
    "            optimizer = Adam(self.parameters(), lr=self.lr)\n",
    "        elif self.opt == 'adamw':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.lr)\n",
    "        elif self.opt == 'rmsprop':\n",
    "            optimizer = RMSprop(self.parameters(), lr=self.lr)\n",
    "        elif self.opt == 'sgd':\n",
    "            optimizer = SGD(self.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Undefined Optimizer.\") \n",
    "        \n",
    "        # Scheduler\n",
    "        if self.sched == None:\n",
    "            return {'optimizer': optimizer}\n",
    "        elif self.sched == 'ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, verbose=True)\n",
    "        elif self.sched == 'LambdaLR':\n",
    "            scheduler = LambdaLR(optimizer, verbose=True)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Undefined Scheduler.\") \n",
    "        \n",
    "        return {'optimizer': optimizer, \n",
    "                'lr_scheduler': scheduler, \n",
    "                'monitor': 'Valid_Loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:21:59.513894Z",
     "start_time": "2021-04-12T17:21:59.504359Z"
    }
   },
   "outputs": [],
   "source": [
    "''' DEFINE THE EXPERIMENT CONFIG FILE '''\n",
    "\n",
    "# Defining various parameters.\n",
    "cfg = {\n",
    "    # Model Parameters\n",
    "    'layers': [2048, 1024, 512, 256],\n",
    "    \n",
    "    # DataLoader Parameters\n",
    "    'batch_size': 1024,\n",
    "    'num_workers': 32,\n",
    "\n",
    "    # Training Parameters\n",
    "    'device': torch.device('cuda:0'),   # Replace with 'cpu' if not using GPU.\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'adam',\n",
    "    'scheduler': None,                  # 'ReduceLROnPlateau',\n",
    "    'model_name': 'model1',\n",
    "    'log_dir': 'logs',\n",
    "    'checkpoint_count': 3,\n",
    "    'callbacks': ['ModelCheckpoint']    # ['ModelCheckpoint', 'EarlyStopping']   \n",
    "}\n",
    "\n",
    "# Config for PyTorch Lightning Trainer.\n",
    "PyTorch_Lightning_Trainer_cfg = {\n",
    "    'max_epochs' : 1000,\n",
    "    'weights_summary' : None,\n",
    "    'gpus': [0],                        # Replace with [] if not using GPU.\n",
    "    'log_every_n_steps': 5,\n",
    "    'check_val_every_n_epoch': 1\n",
    "    \n",
    "    ### Other Trainer settings ###\n",
    "    # 'precision': 16\n",
    "    # 'auto_lr_find': True,\n",
    "    # 'overfit_batches' : 0.001, \n",
    "    # 'val_check_interval': 1.0,\n",
    "    # 'limit_train_batches': 10,\n",
    "    # 'limit_val_batches': 0,\n",
    "    # 'progress_bar_refresh_rate': 0\n",
    "}\n",
    "\n",
    "# Cache in main config for later reference.\n",
    "cfg['PL_CONFIG'] = PyTorch_Lightning_Trainer_cfg\n",
    "cfg['MAX_INSTANCES'] = MAX_INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:36:09.625085Z",
     "start_time": "2021-04-12T17:36:09.615993Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCITONS: CALLBACK AND LOGGER DEFINITIONS '''\n",
    "\n",
    "def get_callbacks(cfg):\n",
    "    ''' Load callbacks specified in the config.\n",
    "    '''\n",
    "    callbacks = []\n",
    "    for callback_ID in cfg['callbacks']:\n",
    "        if callback_ID == 'ModelCheckpoint':\n",
    "            callbacks += [ModelCheckpoint(monitor='Valid_Loss',\n",
    "                                         filename='{epoch:04d}_{Valid_Loss:.3f}',\n",
    "                                         save_top_k=cfg['checkpoint_count'],\n",
    "                                         save_last=True,\n",
    "                                         mode='min', \n",
    "                                         verbose=True)]\n",
    "        elif callback_ID == 'EarlyStopping':\n",
    "            callbacks += [EarlyStopping(monitor='Valid_Loss',\n",
    "                                       min_delta=0.00,\n",
    "                                       patience=20,\n",
    "                                       mode='min',\n",
    "                                       verbose=True)]\n",
    "        else:\n",
    "            raise NotImplementedError('Callback not defined.')\n",
    "    return callbacks\n",
    "\n",
    "def get_logger(cfg):\n",
    "    ''' Load logger using settings specified in the config.\n",
    "    '''\n",
    "    logger = TensorBoardLogger(cfg['log_dir'], name=cfg['model_name'])\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:36:12.534025Z",
     "start_time": "2021-04-12T17:36:10.498537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Datasets: 127999 16001 16000\n",
      "Number of Batches : 125 16 16\n"
     ]
    }
   ],
   "source": [
    "''' DATA PREPERATION '''\n",
    "\n",
    "# Split the dataset.\n",
    "X_train, X_valid, X_test = split_dataset(X)\n",
    "print(\"Length of Datasets:\", len(X_train), len(X_valid), len(X_test))\n",
    "\n",
    "# Get the Data Loaders\n",
    "train_dataLoader = get_dataLoader(X_train, cfg)\n",
    "valid_dataLoader = get_dataLoader(X_valid, cfg)\n",
    "test_dataLoader = get_dataLoader(X_test, cfg)\n",
    "print('Number of Batches :', \n",
    "      len(train_dataLoader), len(valid_dataLoader), len(test_dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:36:13.369476Z",
     "start_time": "2021-04-12T17:36:13.211545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# Define the Pytorch-Lighting trainer using config settings\n",
    "trainer = pl.Trainer(callbacks = get_callbacks(cfg), \n",
    "                     logger = get_logger(cfg),\n",
    "                     **cfg['PL_CONFIG'])\n",
    "\n",
    "# Instantiate the above defined Classifier\n",
    "classifier = Classifier(cfg).to(cfg['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:36:14.807052Z",
     "start_time": "2021-04-12T17:36:14.538863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load previous trained checkpoint for further training.\n",
    "# Is Optional\n",
    "\n",
    "CKPT_PATH = 'logs/model1/version_37/checkpoints/epoch=0108_Valid_Loss=32.932.ckpt'\n",
    "classifier = classifier.load_from_checkpoint(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:36:16.562689Z",
     "start_time": "2021-04-12T17:36:16.559905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer.fit(classifier, train_dataLoader, valid_dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:36:17.946419Z",
     "start_time": "2021-04-12T17:36:17.942578Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance as emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:37:50.301768Z",
     "start_time": "2021-04-12T17:37:50.288409Z"
    }
   },
   "outputs": [],
   "source": [
    "''' FUNCTIONS: GET PREDICTIONS, CALCULATE METRICS '''\n",
    "\n",
    "def get_eval_preds(classifier, trainer, dataLoader):\n",
    "    ''' Get predictions from a trained model (`classifier` + \n",
    "        `trainer`), on a dataset (`dataLoader`). \n",
    "    '''\n",
    "    classifier.predictions = []\n",
    "    training_metrics = trainer.test(classifier, \n",
    "                                    test_dataloaders=dataLoader, \n",
    "                                    verbose=False)\n",
    "    preds = np.array([p.numpy() for p in classifier.predictions])\n",
    "    return preds\n",
    "\n",
    "def get_EMD(preds, X, lim=10000):\n",
    "    ''' Calculate the Earth Mover Distance (EMD) metric between \n",
    "        the ground truth (`X`) and the predictions (`preds`).\n",
    "    '''\n",
    "    EMD = []\n",
    "    lim = min(lim, len(X))\n",
    "    for idx in range(lim):\n",
    "        EMD += [emd(preds[idx], X[idx])]\n",
    "    EMD = np.mean(EMD)\n",
    "    return EMD\n",
    "\n",
    "def evaluate(classifier, trainer, dataLoader, X, split):\n",
    "    ''' Evaluate a model (`classifier` + `trainer`) on a \n",
    "        dataset (`dataLoader` + `X`). Outputs dataset mean, \n",
    "        RMSE and EMD (Earth Mover Distance) metrics.\n",
    "    '''\n",
    "    preds = get_eval_preds(classifier, trainer, dataLoader)\n",
    "    RMSE = mean_squared_error(preds, X, squared=False)\n",
    "    EMD = get_EMD(preds, X)\n",
    "    MEAN = np.mean(X)\n",
    "    print('Data Split:', split)\n",
    "    print('RMSE Error: {:.3f} ± {:.3f}'.format(MEAN, RMSE))\n",
    "    print('EMD: {:.3f}'.format(EMD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T17:38:47.604941Z",
     "start_time": "2021-04-12T17:37:52.457513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48a1d3e7cd4735a858c42ec12921b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split: Train\n",
      "RMSE Error: 24.799 ± 4.078\n",
      "EMD: 1.076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c78bf15dfbd431093077703b485583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split: Valid\n",
      "RMSE Error: 24.852 ± 4.294\n",
      "EMD: 1.115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d414b627cdf04622a5d01aafc4a37287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split: Test\n",
      "RMSE Error: 24.795 ± 4.296\n",
      "EMD: 1.119\n"
     ]
    }
   ],
   "source": [
    "''' LOAD PREVIOUS CHECKPOINT AND EVALUATE IT '''\n",
    "\n",
    "# Load a previous checkpoint for evaluation\n",
    "CKPT_PATH = 'logs/model1/version_37/checkpoints/epoch=0108_Valid_Loss=32.932.ckpt'\n",
    "classifier = classifier.load_from_checkpoint(CKPT_PATH)\n",
    "\n",
    "# Evaluate trained model on different split\n",
    "evaluate(classifier, trainer, train_dataLoader, X_train, 'Train')\n",
    "evaluate(classifier, trainer, valid_dataLoader, X_valid, 'Valid')\n",
    "evaluate(classifier, trainer, test_dataLoader, X_test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:22:53.690992Z",
     "start_time": "2021-04-11T19:22:21.622182Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
